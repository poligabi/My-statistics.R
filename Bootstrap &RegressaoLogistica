

#### BASIC BOOTSTRAP    

# generate hypothetical sample x = {1,2, ... ,8,9}
x = seq(1:9)

# set up a empty matrix to populate Bootstrap draws
B = matrix(nrow = 10, ncol = 9,
           dimnames = list(paste('Bootstrap Sample',1:10), 
                           LETTERS[1:9]))
# loop 10 times
set.seed(111)
for(i in 1:10){
  # draw random samples from x
  B[i,] = sample(x, size = length(x), replace = TRUE) %>% sort()
}

########

# Split treino teste		####https://medium.com/data-hackers/introdu%C3%A7%C3%A3o-ao-tidymodels-em-r-d4188f63c9e1
quakes_split <- initial_split(df, strata = mag)
quakes_split

  # Aqui as divis√µes s√£o transformadas em dataframes, de fato.
  quakes_train <- training(quakes_split)
quakes_teste <- testing(quakes_split)

# Criando booststrap ->  sequ√™ncia de divis√µes no dataset
quakes_boot <- bootstraps(quakes_train)
quakes_boot


#REGRESSAO LOGISTICA   ###https://smolski.github.io/livroavancado/reglog.html
#SIMPLES
require(readr)

m1=glm(Y~Elev, family = binomial(link="logit"), data = test1) #Y~modelo
summary(m1)

# Filtrando a idade dos indiv√≠duos
ELEV <- data.frame(Elev = test1$Elev) 

# Criando campo de predi√ß√£o para cada idade dos indiv√≠duos 
test1$PRED=predict(m1, newdata=ELEV, type="response")

# Plotando a probabilidade predita pelo modelo
require(ggplot2)
ggplot(test1, aes(x=Elev, y=PRED)) + 
  geom_point()


#O modelo de regress√£o log√≠stica traz os resultados dos estimadores na forma logar√≠tma, 
#ou seja, o log das chances da vari√°vel

#exponencia√ß√£o da(s) variavel(eis) da regress√£o. 
#obt√©m-se a raz√£o das chances (OR - Odds Ratio em ingl√™s) para as vari√°veis independentes
require(mfx)
logitor(Y~Elev,data = test1)

#intervalo de confian√ßa
exp(cbind(OR=coef(m1), confint(m1)))

#predi√ß√£o de probabilidades
#chance de ocorrer Y com rela√ß√£o √† uma determinada Elev
mediaE = data.frame(Elev=mean(test1$Elev))
mediaE

mediaE$pred.prob = predict(m1, newdata=mediaE, type="response")
mediaE

#MATRIZ DE CONFUSAO
require(caret)

test1$pdata <- as.factor(
    ifelse(
        predict(m1, 
                newdata = test1, 
                type = "response")
        >0.5,"1","0"))

confusionMatrix(factor(test1$pdata), factor(test1$Y), positive="1")
#We have to keep in mind that both levels should be the same.
#table(factor(pred, levels=min(test):max(test)), factor(test, levels=min(test):max(test)))
# table = confusion matrix





#Regress√£o Logistica MULTIPLA
require(haven)

logit=glm(y_bin~x1+x2+x3, data=mydata, family = binomial(link="logit"))
summary(logit)

require(stargazer)
stargazer(logit, title="Resultados",type = "text")

#raz√£o de chances (OR - odds ratio em ingl√™s) estimada no modelo ter√° de ser transformada por estar apresentada na forma logar√≠tma
require(mfx)
logitor(y_bin~x1+x2+x3,data=mydata)

#transformado a sua exponencia√ß√£o dos coeficientes log√≠sticos 
exp(coef(logit))

#intervalo de confian√ßa
exp(cbind(OR=coef(logit), confint(logit)))

## Predi√ß√µes das probabilidades
allmean = data.frame(x1=mean(mydata$x1),
                     x2=mean(mydata$x2),
                     x3=mean(mydata$x3))
allmean

allmean$pred.prob = predict(logit, newdata=allmean, type="response")
allmean

## M√©todo Stepwise
#selecionar as vari√°veis importantes ao modelo, 
#podem ser utilizadas nas dire√ß√µes ‚Äúboth‚Äù, ‚Äúbackward‚Äù, ‚Äúforward‚Äù. 
#utiliza o Crit√©rio de Informa√ß√£o de Akaike - AIC (menor=melhor)
step(logit, direction = 'both')


# Hair et al. (2009, 191):
#‚Äúmulticolinearidade cria vari√¢ncia ‚Äúcompartilhada‚Äù entre vari√°veis, diminuindo assim a capacidade de prever a medida dependente, bem como averiguar os pap√©is relativos de cada vari√°vel independente"
## Teste do fator de infla√ß√£o da vari√¢ncia (VIF - Variance Inflation Factor)
#√≠ndice n√£o deve ficar abaixo de 10 para representar baixo problema de multicolinearidade segundo Rawlings, Pantula, e Dickey (1998)

require(faraway)
vif(logit)



#######BOOTSTRAPPING REGRESSION MODELS  #https://docs.ufpr.br/~taconeli/CE22518/Bootstrap.pdf

regression_object=glm(Y~modelo, family=binomial(link="logit"))

Boot(regression_object, f, labels, R = 999, method)
#f= coef (default= salva coeficientes de cada replica)
#labels=c(labels(coef(mod)), "sigmaHat") if f returns both coeficient estimates and the scale estimate
#method="case"(the default for case resampling)
#method="residual"(ex: experiments, fix pairs of variables and resample the residuals 

require(car)
require(boot)

rm1 <- rlm(Y ~ Elev, data=test1, maxit=200)
rm1.boot <- Boot(rm1, R=1999)

summary(rm1.boot, high.moments=TRUE)


confint(rm1.boot, level=.90, type="norm")

hist(rm1.boot, legend="separate")

#Figure 3: Jackknife-after-bootstrap plot for the elevation (a) and ground slope (b) coecients in the
# regression for monkeys' occupation data.
jack.after.boot(rm1.boot, index=2, main="(a) Elevation")

plot(rm1.boot)



#### https://github.com/serafimpetrov1/bootstrap/blob/main/BootReg.R

#Predicting on new data
new.data = seq(min(x), max(x), by = 0.05)
conf_interval <-
  predict(
    sample.model,                                      #sample.model <- lm(y ~ x, data = sample.data)
    newdata = data.frame(x = new.data),
    interval = "confidence",
    level = 0.95)

 #coefs <- rbind(sample_coef_intercept, sample_coef_x1) 
str(rm1.boot)         #pra descobrir que colunas s√£o essas
coefs <- rbind(rm1.boot$_coef_intercept, rm1.boot$_coef_x1)


#Plotting the results on the project step-by-spet
plot(
  y ~ x,
  col = "gray",
  xlab = "x",
  ylab = "y",
  main = "Compare regressions")
apply(coefs, 2, abline, col = rgb(1, 0, 0, 0.03))                           
abline(coef(population.model)[1], coef(population.model)[2], col = "blue")              #population.model <- lm(y ~ x, population.data)
abline(coef(sample.model)[1],
       coef(sample.model)[2],
       col = "black",
       lty = 2, lwd=3)
abline(mean(sample_coef_intercept),                        #model_bootstrap$coefficients[1]
       mean(sample_coef_x1),                                      #model_bootstrap$coefficients[2]
       col = "green",
       lty = 4, lwd=3)
lines(new.data, conf_interval[, 2], col = "black", lty = 3, lwd=3)           #new.data = seq(min(x), max(x), by = 0.05)
lines(new.data, conf_interval[, 3], col = "black", lty = 3, lwd=3)
legend("topleft",
       legend = c("Bootstrap", "Population", 'Sample'),
       col = c("red", "blue", 'green'),
       lty = 1:3,
       cex = 0.8)



#https://rpubs.com/vadimus/bootstrap
#This allows the user to handle multiple bootstrap samples in parallel, 
#which cuts down on processing time and 
#allows to keep the number of repetitions R sufficiently large
library(boot)
library(parallel)

# function to return bootstrapped coefficients
myLogitCoef <- function(data, indices, formula) {
  d <- data[indices,]
  fit <- glm(formula, data=d, family = binomial(link = "logit"))
  return(coef(fit))
}

# set up cluster of 4 CPU cores
cl<-makeCluster(4)
clusterExport(cl, 'myLogitCoef')

set.seed(373)
coef.boot <- boot(data=data, statistic=myLogitCoef, R=1000, 
                  formula= logit$formula,
                  # process in parallel across 4 CPU cores
                  parallel = 'snow', ncpus=4, cl=cl)
stopCluster(cl)






##### BOOTSTRAP CONFIDENCE INTERVAL #####
###  (comparing 2 numeric variables)  ### https://www.youtube.com/watch?v=Om5TMGj9td4

# load in the chicken diet data (save it in "d")
d <- read.table(C:/R/ChickData.csv", header=T, sep="," )
# this data is a subset of the "chickwts" data in the
# "R datasets package"

# let's add the data into the "data view"
View(d)

# check the names, etc
names(d)
levels(d$feed)
# how many observations in each diet?
table(d$feed)

# let's look at a boxplot of weight gain by those 2 diets
boxplot(d$weight~d$feed, las=1, ylab="weight (g)", 
        xlab="feed",main="Weight by Feed")

# calculate the difference in sample means
mean(d$weight[d$feed=="casein"])  # mean for casein
mean(d$weight[d$feed=="meatmeal"])  # mean for meatmeal
# and, a fancier way to do that...
with(d, tapply(weight, feed, mean))
# lets calculate the diff in means:   (casein - meatmeal)
Obs.Diff.In.Means <- (mean(d$weight[d$feed=="casein"]) - mean(d$weight[d$feed=="meatmeal"]))  #diff in means
Obs.Diff.In.Means
# and, a fanceir way to do that...  (- to have it be casein-meatmeal)
-diff( with(d, tapply(weight, feed, mean)) ) 

# and, the same for the medians
median(d$weight[d$feed=="casein"])  # median for casein
median(d$weight[d$feed=="meatmeal"])  # median for meatmeal
# and, a fancier way to do that...
with(d, tapply(weight, feed, median))
# lets calculate the diff in medians:  (casein - meatmeal)
Obs.Diff.In.Medians <- (median(d$weight[d$feed=="casein"]) - median(d$weight[d$feed=="meatmeal"]))  #diff in medians
Obs.Diff.In.Medians
# and, a fanceir way to do that...  (- to have it be casein-meatmeal)
-diff( with(d, tapply(weight, feed, median)) ) 

###################################
### BOOTSTRAP CONFIDENCE INTERVAL
###################################

# let's run through making conf ints for the difference in means and medians

# let's bootstrap...
set.seed(13579)   # set a seed for consistency/reproducability
n.c <- 12  # the number of observations to sample from casein
n.m <- 11  # the number of observations to sample from meatmeal
B <- 100000  # the number of bootstrap samples...go big or go home right?

# now, get those bootstrap samples (without loops!)
# stick each Boot-sample in a column...
Boot.casein <- matrix( sample(d$weight[d$feed=="casein"], size= B*n.c, 
                              replace=TRUE), ncol=B, nrow=n.c)
Boot.meatmeal <- matrix( sample(d$weight[d$feed=="meatmeal"], size= B*n.m, 
                                replace=TRUE), nrow=n.m, ncol=B)
# check those
dim(Boot.casein); dim(Boot.meatmeal)

# check to make sure they are not empty!
Boot.casein[1:5,1:5]
Boot.meatmeal[1:5,1:5]

# calculate the difference in MEANS for each of the bootsamples
Boot.Diff.In.Means <- colMeans(Boot.casein) - colMeans(Boot.meatmeal)
# check that
length(Boot.Diff.In.Means)
# and, look at the first 10 diff in means
Boot.Diff.In.Means[1:10]

# calculate the difference in MEDIANS for each of the bootsamples
Boot.Diff.In.Medians <- apply(Boot.casein, MARGIN=2, FUN=median) -
  apply(Boot.meatmeal, MARGIN=2, FUN=median)
# check that
length(Boot.Diff.In.Medians)
# and, look at the first 10 diff in medians
Boot.Diff.In.Medians[1:10]

#### MAKE THE CONFIDENCE INTERVALS (using 95% confidence)

# let's look at the PERCENTILE METHOD
# the "PERCENTILE" bootstrap confidence interval
# first, for the difference in MEANS
quantile(Boot.Diff.In.Means, prob=0.025)
quantile(Boot.Diff.In.Means, prob=0.975)

# and then, the difference in MEDIANS
quantile(Boot.Diff.In.Medians, prob=0.025)
quantile(Boot.Diff.In.Medians, prob=0.975)

### What do you make of the fact that these both cross 0?

### Apart from "statistical significance", what do you think about
###    "scientific significance" here?

# below is code to calculate confidence interval using the BASIC method

# let's look at the BASIC METHOD
# first, for the difference in MEANS
2*Obs.Diff.In.Means - quantile(Boot.Diff.In.Means, prob=0.975)
2*Obs.Diff.In.Means - quantile(Boot.Diff.In.Means, prob=0.025)
# and then, the difference in MEDIANS
2*Obs.Diff.In.Medians - quantile(Boot.Diff.In.Medians, prob=0.975)
2*Obs.Diff.In.Medians - quantile(Boot.Diff.In.Medians, prob=0.025)

########
### Code for confidence interval for difference in 80th percentiles
########

# calculate the observed difference in 80th percentiles
Obs.Diff.In.80per <- (quantile(d$weight[d$feed=="casein"], prob=0.80) - quantile(d$weight[d$feed=="meatmeal"], prob=0.80))
Obs.Diff.In.80per

# calculate the difference in 80th percentile for each of the bootsamples
Boot.Diff.In.80per <- apply(Boot.casein, MARGIN=2, FUN=quantile, prob=0.80) - apply(Boot.meatmeal, MARGIN=2, FUN=quantile, prob=0.80)

# let's look at the PERCENTILE METHOD for the difference in 80th percentile
quantile(Boot.Diff.In.80per, prob=0.025)
quantile(Boot.Diff.In.80per, prob=0.975)
