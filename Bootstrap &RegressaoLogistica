

#### BASIC BOOTSTRAP    

# generate hypothetical sample x = {1,2, ... ,8,9}
x = seq(1:9)

# set up a empty matrix to populate Bootstrap draws
B = matrix(nrow = 10, ncol = 9,
           dimnames = list(paste('Bootstrap Sample',1:10), 
                           LETTERS[1:9]))
# loop 10 times
set.seed(111)
for(i in 1:10){
  # draw random samples from x
  B[i,] = sample(x, size = length(x), replace = TRUE) %>% sort()
}


###########

# Split treino teste            ##https://medium.com/data-hackers/introdu%C3%A7%C3%A3o-ao-tidymodels-em-r-d4188f63c9e1
quakes_split <- initial_split(df, strata = mag)
quakes_split

  # Aqui as divisões são transformadas em dataframes, de fato.
  quakes_train <- training(quakes_split)
quakes_teste <- testing(quakes_split)

# Criando booststrap ->  sequência de divisões no dataset
quakes_boot <- bootstraps(quakes_train)
quakes_boot


#REGRESSAO LOGISTICA   ###https://smolski.github.io/livroavancado/reglog.html
#SIMPLES
require(readr)

m1=glm(Y~Elev, family = binomial(link="logit"), data = test1) #Y~modelo
summary(m1)

### Filtrando a idade dos indivíduos
ELEV <- data.frame(Elev = test1$Elev) 

# Criando campo de predição para cada idade dos indivíduos 
test1$PRED=predict(m1, newdata=ELEV, type="response")

# Plotando a probabilidade predita pelo modelo
require(ggplot2)
ggplot(test1, aes(x=Elev, y=PRED)) + 
  geom_point()


#O modelo de regressão logística traz os resultados dos estimadores na forma logarítma, 
#ou seja, o log das chances da variável

#exponenciação da(s) variavel(eis) da regressão. 
#obtém-se a razão das chances (OR - Odds Ratio em inglês) para as variáveis independentes
require(mfx)
logitor(Y~Elev,data = test1)

#intervalo de confiança
exp(cbind(OR=coef(m1), confint(m1)))

## Predição de Probabilidades
#chance de ocorrer Y com relação à uma determinada Elev  
mediaE = data.frame(Elev=mean(test1$Elev))             ##não pode ter NA
mediaE

mediaE$pred.prob = predict(m1, newdata=mediaE, type="response")
mediaE

## MATRIZ DE CONFUSAO                         ##usa dados em forma de fator
require(caret)

test1$pdata <- as.factor(
    ifelse(
        predict(m1, 
                newdata = test1, 
                type = "response")
        >0.5,"1","0"))

confusionMatrix(factor(test1$pdata), factor(test1$Y), positive="1")
##We have to keep in mind that both levels should be the same.
#table(factor(pred, levels=min(test):max(test)), factor(test, levels=min(test):max(test)))
# table = confusion matrix



#Regressão Logistica MULTIPLA
require(haven)

logit=glm(y_bin~x1+x2+x3, data=mydata, family = binomial(link="logit"))
summary(logit)

require(stargazer)
stargazer(logit, title="Resultados",type = "text")

#razão de chances (OR - odds ratio em inglês) estimada no modelo terá de ser transformada por estar apresentada na forma logarítma
require(mfx)
logitor(y_bin~x1+x2+x3,data=mydata)

#transformado a sua exponenciação dos coeficientes logísticos 
exp(coef(logit))

#intervalo de confiança
exp(cbind(OR=coef(logit), confint(logit)))

## Predições das probabilidades
allmean = data.frame(x1=mean(mydata$x1),
                     x2=mean(mydata$x2),
                     x3=mean(mydata$x3))
allmean

allmean$pred.prob = predict(logit, newdata=allmean, type="response")
allmean

## Método Stepwise
#selecionar as variáveis importantes ao modelo, 
#podem ser utilizadas nas direções “both”, “backward”, “forward”. 
#utiliza o Critério de Informação de Akaike - AIC (menor=melhor)
step(logit, direction = 'both')


# Hair et al. (2009, 191):
#“multicolinearidade cria variância “compartilhada” entre variáveis, diminuindo assim a capacidade de prever a medida dependente, bem como averiguar os papéis relativos de cada variável independente"
## Teste do fator de inflação da variância (VIF - Variance Inflation Factor)
#índice não deve ficar abaixo de 10 para representar baixo problema de multicolinearidade segundo Rawlings, Pantula, e Dickey (1998)

require(faraway)
vif(logit)






#######BOOTSTRAPPING REGRESSION MODELS  #https://docs.ufpr.br/~taconeli/CE22518/Bootstrap.pdf

regression_object=glm(Y~modelo, family=binomial(link="logit"))

Boot(regression_object, f, labels, R = 999, method)
#f= coef (default= salva coeficientes de cada replica)
#labels=c(labels(coef(mod)), "sigmaHat") if f returns both coeficient estimates and the scale estimate
#method="case"(the default for case resampling)
#method="residual"(ex: experiments, fix pairs of variables and resample the residuals 




#https://rpubs.com/vadimus/bootstrap
#This allows the user to handle multiple bootstrap samples in parallel, 
#which cuts down on processing time and 
#allows to keep the number of repetitions R sufficiently large
library(boot)
library(parallel)

# function to return bootstrapped coefficients
myLogitCoef <- function(data, indices, formula) {
  d <- data[indices,]
  fit <- glm(formula, data=d, family = binomial(link = "logit"))
  return(coef(fit))
}

# set up cluster of 4 CPU cores
cl<-makeCluster(4)
clusterExport(cl, 'myLogitCoef')

set.seed(373)
coef.boot <- boot(data=data, statistic=myLogitCoef, R=1000, 
                  formula= logit$formula,
                  # process in parallel across 4 CPU cores
                  parallel = 'snow', ncpus=4, cl=cl)
stopCluster(cl)









##### BOOTSTRAP CONFIDENCE INTERVAL #####
###  (comparing 2 numeric variables)  ### https://www.youtube.com/watch?v=Om5TMGj9td4

# load in the chicken diet data (save it in "d")
d <- read.table(C:/R/ChickData.csv", header=T, sep="," )
# this data is a subset of the "chickwts" data in the
# "R datasets package"

# let's add the data into the "data view"
View(d)

# check the names, etc
names(d)
levels(d$feed)
# how many observations in each diet?
table(d$feed)

# let's look at a boxplot of weight gain by those 2 diets
boxplot(d$weight~d$feed, las=1, ylab="weight (g)", 
        xlab="feed",main="Weight by Feed")

# calculate the difference in sample means
mean(d$weight[d$feed=="casein"])  # mean for casein
mean(d$weight[d$feed=="meatmeal"])  # mean for meatmeal
# and, a fancier way to do that...
with(d, tapply(weight, feed, mean))
# lets calculate the diff in means:   (casein - meatmeal)
Obs.Diff.In.Means <- (mean(d$weight[d$feed=="casein"]) - mean(d$weight[d$feed=="meatmeal"]))  #diff in means
Obs.Diff.In.Means
# and, a fanceir way to do that...  (- to have it be casein-meatmeal)
-diff( with(d, tapply(weight, feed, mean)) ) 

# and, the same for the medians
median(d$weight[d$feed=="casein"])  # median for casein
median(d$weight[d$feed=="meatmeal"])  # median for meatmeal
# and, a fancier way to do that...
with(d, tapply(weight, feed, median))
# lets calculate the diff in medians:  (casein - meatmeal)
Obs.Diff.In.Medians <- (median(d$weight[d$feed=="casein"]) - median(d$weight[d$feed=="meatmeal"]))  #diff in medians
Obs.Diff.In.Medians
# and, a fanceir way to do that...  (- to have it be casein-meatmeal)
-diff( with(d, tapply(weight, feed, median)) ) 

###################################
### BOOTSTRAP CONFIDENCE INTERVAL
###################################

# let's run through making conf ints for the difference in means and medians

# let's bootstrap...
set.seed(13579)   # set a seed for consistency/reproducability
n.c <- 12  # the number of observations to sample from casein
n.m <- 11  # the number of observations to sample from meatmeal
B <- 100000  # the number of bootstrap samples...go big or go home right?

# now, get those bootstrap samples (without loops!)
# stick each Boot-sample in a column...
Boot.casein <- matrix( sample(d$weight[d$feed=="casein"], size= B*n.c, 
                              replace=TRUE), ncol=B, nrow=n.c)
Boot.meatmeal <- matrix( sample(d$weight[d$feed=="meatmeal"], size= B*n.m, 
                                replace=TRUE), nrow=n.m, ncol=B)
# check those
dim(Boot.casein); dim(Boot.meatmeal)

# check to make sure they are not empty!
Boot.casein[1:5,1:5]
Boot.meatmeal[1:5,1:5]

# calculate the difference in MEANS for each of the bootsamples
Boot.Diff.In.Means <- colMeans(Boot.casein) - colMeans(Boot.meatmeal)
# check that
length(Boot.Diff.In.Means)
# and, look at the first 10 diff in means
Boot.Diff.In.Means[1:10]

# calculate the difference in MEDIANS for each of the bootsamples
Boot.Diff.In.Medians <- apply(Boot.casein, MARGIN=2, FUN=median) -
  apply(Boot.meatmeal, MARGIN=2, FUN=median)
# check that
length(Boot.Diff.In.Medians)
# and, look at the first 10 diff in medians
Boot.Diff.In.Medians[1:10]

#### MAKE THE CONFIDENCE INTERVALS (using 95% confidence)

# let's look at the PERCENTILE METHOD
# the "PERCENTILE" bootstrap confidence interval
# first, for the difference in MEANS
quantile(Boot.Diff.In.Means, prob=0.025)
quantile(Boot.Diff.In.Means, prob=0.975)

# and then, the difference in MEDIANS
quantile(Boot.Diff.In.Medians, prob=0.025)
quantile(Boot.Diff.In.Medians, prob=0.975)

### What do you make of the fact that these both cross 0?

### Apart from "statistical significance", what do you think about
###    "scientific significance" here?

# below is code to calculate confidence interval using the BASIC method

# let's look at the BASIC METHOD
# first, for the difference in MEANS
2*Obs.Diff.In.Means - quantile(Boot.Diff.In.Means, prob=0.975)
2*Obs.Diff.In.Means - quantile(Boot.Diff.In.Means, prob=0.025)
# and then, the difference in MEDIANS
2*Obs.Diff.In.Medians - quantile(Boot.Diff.In.Medians, prob=0.975)
2*Obs.Diff.In.Medians - quantile(Boot.Diff.In.Medians, prob=0.025)

########
### Code for confidence interval for difference in 80th percentiles
########

# calculate the observed difference in 80th percentiles
Obs.Diff.In.80per <- (quantile(d$weight[d$feed=="casein"], prob=0.80) - quantile(d$weight[d$feed=="meatmeal"], prob=0.80))
Obs.Diff.In.80per

# calculate the difference in 80th percentile for each of the bootsamples
Boot.Diff.In.80per <- apply(Boot.casein, MARGIN=2, FUN=quantile, prob=0.80) - apply(Boot.meatmeal, MARGIN=2, FUN=quantile, prob=0.80)

# let's look at the PERCENTILE METHOD for the difference in 80th percentile
quantile(Boot.Diff.In.80per, prob=0.025)
quantile(Boot.Diff.In.80per, prob=0.975)
