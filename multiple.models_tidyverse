
### Starting with tidyverse Books:
#https://r4ds.had.co.nz/data-import.html
#https://www.tidymodels.org/start/models/
#https://moderndive.com/4-tidy.html
#https://supervised-ml-course.netlify.app/



##################################################################

######## TIDY DATA (transformando colunas excessivas de uma mesma variável em 2 coluna, 1 com os valores e outra com os nomes das colunas
data_tidy <- data %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "score",   #conterá os valores
               values_drop_na = TRUE  #expliciar que há ausencias nos dados
               cols = -local,         #tidy todas as colunas menos essa ou:
               #cols = c(3:40),
               names_transform = list(year = as.integer)) #assim os anos que são números, serão lidos como valores e não texto
  #Há tbm o inverso, se você coloca uma coluna para tipos de variáveis (ex:escalas) que deveriam ter cada uma sua própia coluna
  table2 %>%
    pivot_wider(names_from = escalas, values_from = count)

####### filtrando              
drinks_smaller <- drinks %>% 
  filter(country %in% c("USA", "China", "Italy", "Saudi Arabia")) %>% #filtra so esses países
  select(-total_litres_of_pure_alcohol) %>%                           #seleciona todas col menos essa
  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)      #renomeia pra nome mais facil

###### To COMBINE the tidied versions of table4a and table4b into a single tibble dplyr::left_join()
tidy4a <- table4a %>%  #primeiro tidied
  pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "cases")
tidy4b <- table4b %>% 
  pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "population")
left_join(tidy4a, tidy4b, by="local") #depois combina
      #it preserves the original observations even when there isn’t a match. The left join should be your default join  
  ####### RELATIONAL DATA (multiplas tabelas com dados relacionados ex: matrix de sp e de ambientais
  #https://r4ds.had.co.nz/relational-data.html
  ### KEY variavel que identifica a observação unica
#An inner join keeps observations that appear in both tables
x %>% 
  inner_join(y, by = "key")
#A left join keeps all observations in x(add NA on col y). Default pois add mantendo a quantidade de linhas original
#A right join keeps all observations in y(add NA on col x).
#A full join keeps all observations in x and y (add lines to do it).
#by = NULL, uses all variables that appear in both tables, the so called natural join (default)
#by = c("a" = "b"). This will match variable a in table x to variable b in table y (2col que representam =var mas com nomes =!)





################
# Plot Riqueza #
################

# Criar 3 objetos (variáveis espaciais, ambientais e dados das espécies)

listaAL<- read.table(file = "clipboard", sep = "\t", header=TRUE)# row.names=1)

spa<-listaAL[,2:3]
env<- listaAL[,4:15]
spe<- listaAL[,18:38]
#str(tax)
#spe<- tax[,-c(8,9,11,12,14)]
library("vegan")

str(spe) #conferindo dados
head(spa) 
str(env)  #podes conferir usando summary(), mas não é muito pratico qnd os dados tem muitas colunas
 
plot(spa$X, spa$Y) #autor sugere ir add camadas em camadas, nesse caso, com dados espaciais, vemos só os pontos do rio

plot(spa$X, spa$Y, type="n")    #type=n gera gráfico sem nada, pontos estão lá mas invisíveis, dai vais add caract dele nas funções abaixo
#lines(spa$X, spa$Y, col="blue4", lwd=2)     #slwd= grossura da linha, eleciona todos juntos no script e executa (ctrlR), ou add + 
text(spa$X, spa$Y, row.names(spa), col="red")   #add rotulo de cada variável


# Add riqueza d sp.
riqueza<-specnumber(spe) #função do pacote q calcula riqueza através dos dados brutos

plot(spa$X, spa$Y, type="n", #ylim=c(20,120),    #função concatenar foi usada p ...
xlab= "Coordenada X(km)", ylab= "Coordenada Y(km)", las=1) 
#lines(spa$X, spa$Y, col="blue4") +
points(spa$X, spa$Y, cex=riqueza,pch=16, col="gray")  #add círculos de riqueza (dividiu por 3.5 pois riqueza ficava muito grande no grafico)
points(spa$X, spa$Y, cex=riqueza,pch=1) +   #add bordas, permite vermos as sobreposições dos círculos
points(spa$X, spa$Y, cex=0.5, pch=16)   #add centros


############################
# Ordenação -> PCA da Caça #
############################

setwd("C:/R/Dout-modelar")
spa<-read.csv("DoubsSpa.csv", row.names=1) 
env<-read.csv("DoubsEnv.csv", row.names=1)
spe<-read.csv("DoubsSpe.csv", row.names=1)
str(env)
hunt <- env[,9:12]
str(hunt)
edit(hunt)

pca.hunt<-rda(hunt, scale=T) 
#scale=T é oq vai padronizar (standardization) os dados, sem ele será usado a matriz de covariancia
#só nao precisa dele qnd todas as variáveis tem a mesma unidade
summary(pca.hunt)
#'cumulative proportion' quanta explicação é fornecida por n Componentes Principais (PC2=olhar p 2 componentes, é até onde vamos usar, par a par)
#'species scores' qnt informação cada variável tras p explica cada Componente Principal (+valor, +explica)
# valores do componente (site scores) pode ser usado como variável

biplot(pca.hunt)
# grafico PC1 x PC2, cada linha vira um ponto
# as variáveis fisico-químicas são expressas em linhas de explicação (inclinação indicada no 'species scores')
# +prox =+correlacionadas, +perpendiculares=+independente das outras

str(pca.hunt) #mostra resultado por dentro
pca.hunt$CA$eig  #chama só os autovalores 'eigenvalue'
pca.hunt$CA$v  #Valores das variáveis 'species scores'
pca.hunt$CA$u  #Coordenadas das unidades amostrais 'site scores'
#armazenar os 2 primeiros componentes (se quiser fazer... ex regressão)
pc1<-pca.hunt$CA$u[,1]
pc2<-pca.hunt$CA$u[,2]

#exemplo tosco:
riqueza<-specnumber(spe)
plot(riqueza~pc1)
abline(lm(riqueza~pc1), col = "blue")

summary

hunt1<- cbind(hunt, pc1)

# correlation
corr <- cor(hunt1, method = "spearman")
corr

data <- listaAL

data<- cbind(listaAL, new_col=pc1)
names(data)[names(data) == 'new_col'] <- 'Huntpc1'
str(data)


#########################################      
## Autocorrelogramas de Mantel      
      
jac.spa<-mantel.correlog(spe.jac, spa.euc) #aqui a ordem importa, a nao ser q escreva por extenso, primeiro argumennto: d.eco= segundo: d.geo=
jac.spa	#cria classes de distância (class.index= até 9m, até 25m, até 42m...), 
#p cada classe indica o n presente nelas (qnd muito baixo o n não consegue calcular alto correlação)
#há autocorrelação qnd Pr(mantel) menor q 0.05
plot(jac.spa) #quadradinho preto autorrelação presente, branco ausente(não signif) ou não calculado
#quando a linha vermelha cruza a linha dos resultado é o ponto ideal que não há autocorrelação espacial, seria a distância entre coletores q deves usar na sua coleta de dados


###########################################################################
##########################  GLMM com os taxons como variáveis aleatorias #
			 ##################################################
str(data)
library("tidyverse")

data_tidy <- data %>% 
  pivot_longer(names_to = "taxon",     #conterá os nomes das colunas orig
               values_to = "occu",   #conterá os valores
               #values_drop_na = TRUE  #expliciar que há ausencias nos dados
               cols = c(20:40))
               #names_transform = list(year = as.integer)) 
str(data_tidy)
readr::write_csv(tibble::as_tibble(data_tidy), "data_tidy.csv")

area <- data_tidy[,"arekm2"]
hunt <- data_tidy[,"Huntpc1"]

means <- c(apply(cbind(area,hunt), 2, mean, na.rm=T)) 
sds <- c(apply(cbind(area,hunt), 2, sd, na.rm=T))

# Scale covariates  => ATENCAO: ordem das medias= ordem usada na funcao acima
areas <- (area - means[1]) / sds[1] #conferir para ter certeza de usar os valores certos
hunts <- (hunt- means[2]) / sds[2]

str(areas)

datamm<-data_tidy
datamm<- cbind(data_tidy, new_col=c(areas, hunts))
str(datamm)
names(datamm)[names(datamm) == c('new_col.arekm2','new_col.Huntpc1')] <- c('areas','hunts')
#arehunpad<-decostand(areahun, method="standardize")

slope.orig <- data_tidy[,"DeclivG"]
m1991.orig <- data_tidy[,"ndvi1991.2"]
m2021.orig<- data_tidy[,"ndvi2021.2"]
area.orig<- data_tidy[,"logarea"]
prot.orig <- data_tidy[,"Protection"]
prec.orig <- data_tidy[,"prec"]
human.density.orig <- data_tidy[,"Hdens"]
nd1991.orig <- data_tidy[,"ndvi1991.5"]
nd2021.orig<- data_tidy[,"ndvi2021.5"]
neig15.orig <- data_tidy[,"neigop_pro1.5"]
taxons<-data_tidy[,"taxon"]
occus<-data_tidy[,"occu"]

# Compute means and standard deviations
means <- c(apply(cbind(slope.orig,m1991.orig,m2021.orig,area.orig,
prot.orig,prec.orig,human.density.orig, nd1991.orig, nd2021.orig,
neig15.orig), 2, mean, na.rm=T)) 
sds <- c(apply(cbind(slope.orig,m1991.orig,m2021.orig,area.orig,
prot.orig,prec.orig,human.density.orig, nd1991.orig, nd2021.orig,
neig15.orig), 2, sd, na.rm=T))
#na.rm=T esse codigo permite q a presença de NA não estrague o calculo 


# Scale covariates  => ATENCAO: ordem das medias= ordem usada na funcao acima

slope <- (slope.orig - means[1]) / sds[1] #conferir para ter certeza de usar os valores certos
m291 <- (m1991.orig - means[2]) / sds[2]
m221 <- (m2021.orig - means[3]) / sds[3]
area <- (area.orig - means[4]) / sds[4]
protec <- (prot.orig - means[5]) / sds[5]
precip <- (prec.orig - means[6]) / sds[6]
hdens <- (human.density.orig - means[7]) / sds[7]
m591 <- (nd1991.orig - means[8]) / sds[8]
m521 <- (nd2021.orig - means[9]) / sds[9]
nprop <- (neig15.orig - means[10]) / sds[10]

#arehun<-data.frame(area,hunt)
#arehunpad<-decostand(areahun, method="standardize")


datag<-data.frame(taxons, occus, 
slope,m291,m221,area,protec,precip,hdens,m591,m521,nprop)
str(datag)
tb <- as_tibble(datag)
tb <- rename(tb, slope=DeclivG,m291= ndvi1991.2,m221=ndvi2021.2,
area=logarea,protec=Protection,precip=prec,hdens=Hdens,
m591=ndvi1991.5,m521=ndvi2021.5,nprop= neigop_pro1.5)
str(tb)


###################
# GLMM 

library(lme4)	#p GLM
library(car) 	#para Anova
#covariates (fixed)
#taxon(random)
#occu (binomial)

#variance +- stdDev: dado para tabela-> mostra quanto nosso dado varia
# estimate +- SE: estimate>SE=signif. se negativo:valores sp2 menores q sp1


n.occu=glmer(datamm$occu~1+(1|datamm$taxon),family=binomial) #glmer p binomial e poisson
m.occu=glmer(datamm$occu~datamm$areas+datamm$hunts+(1|datamm$taxon),family=binomial)

n.occu=glmer(data_tidy$occu~1+(1|data_tidy$taxon),family=binomial) #glmer p binomial e poisson
m.occu=glmer(tb$occu~tb$slope+tb$m291+tb$m221+tb$area+tb$protec+tb$precip+tb$hdens+tb$m591+tb$m521+tb$nprop+(1|tb$taxon),family=binomial)

anova(n.occu,m.occu,test="Chisq")
summary(m.occu)	#para dist binomial test = z, do summary
#se usamos AIC e assumimos dist normal = precisamos usar anova e ir comparando diversas combinações
#se usamos AIC e dist binomial/poisson = obtemos resultado no summary
#no lugar de AIC= fazemos um resumo de todos de modelos com Anova q mostra Chisq
#Anova = nao compara todas as categorias, nao divide A por B, é um resumo

fixef(m.occu)
ranef(m.occu) #chamar coeficientes das variáveis aleatorias (cada taxon)

condm<-as.data.frame(ranef(m.occu)) #$condsd  standard deviation condicional











###############################################################################################################

######## https://juliasilge.com/blog/crop-yields/
######## How mammals richness are changing|vary with changing NDVI
#chamando dados
dt500<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt1m<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt2m<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt5m<- read.table(file = "clipboard", sep = "\t", header=TRUE) #com coluna Nsp
lista<- read.table(file = "clipboard", sep = "\t", header=TRUE)
str(lista) #4=UC_area 8=Nsp
#sp<-lista[,9:109]
#riq<-lista[,c(7,8)]

library(tidyverse)

############################### Tidy the serial data

dt500_td <- dt500 %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "500m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt1m_td <- dt1m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "1000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt2m_td <- dt2m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "2000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(,X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt5m_td <- dt5m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "5000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(,X1984:X2021))
               #names_transform = list(year = as.integer)) 
data_td1<- left_join(dt500_td,dt1m_td) #depois combina
data_td2<- left_join(data_td1,dt2m_td)
data_td5<- left_join(data_td2,dt5m_td)
str(data_td5)
data_td5 <- data_td5 %>% mutate(year = as.integer(gsub("X", "", year))) #tira X dos numeros e torna dados numericos

serialdt <- data_td5 %>% 
  pivot_longer(names_to = "escalas",     #conterá os nomes das colunas orig
               values_to = "ndvi",   #conterá os valores
               cols = c("500m","1000m","2000m","5000m")) %>%
		filter( escalas %in% c("500m","1000m","2000m","5000m"),
			 !is.na(ndvi)) #filtro importante pro modelo a seguir
edit(serialdt)

###################################### Modelo Serial

library(tidymodels)#boom de pacotes conflituosos
library(modelr)#ou um ou outro

serial_lm<- serialdt %>%
  nest(yndvi = c(year, ndvi)) %>% #cria um tible pra cada conjunto de anos e ndvi
  mutate(model = map(yndvi, ~ lm(ndvi ~ year, data = .x)))
	#try() forca ignorar o erro, inviavibiliza criar o modelo
	#http://adv-r.had.co.nz/Exceptions-Debugging.html #olhar tbm tryCatch()

slopes <- serial_lm %>%
  mutate(coefs = map(model, tidy)) %>%
  unnest(coefs) %>%
  filter(term == "year") %>%            
  mutate(p.value = p.adjust(p.value))

################################### Gráficos
library(ggrepel)
slopes %>%
  ggplot(aes(estimate, Nsp, label="")) +  
  geom_vline(
    xintercept = 0, lty = 2,
    size = 1.5, alpha = 0.7, color = "gray50"
  ) +
  geom_point(aes(color = escalas), alpha = 0.8, size = 2.5, show.legend = FALSE) +
  scale_y_log10() +
  facet_wrap(~escalas) +            
  geom_text_repel(size = 3, family = "IBMPlexSans") +
  theme_light(base_family = "IBMPlexSans") +
  theme(strip.text = element_text(family = "IBMPlexSans-Bold", size = 12)) +
  labs(x = "increase in NDVI rate per year")

############## with mean NDVI




##############################################################################

#########https://juliasilge.com/blog/bird-baths/

lista_tidy <- data %>% 
  pivot_longer(names_to = "sp",     #conterá os nomes das colunas orig
               values_to = "pres",   #conterá os valores
               values_drop_na = TRUE  #expliciar que há ausencias nos dados
               #cols = -local,         #tidy todas as colunas menos essa ou:
               cols = c(sp1:spx))
#tipoUCN ou preaus medium monkey 
lista_df <-
  lista_tidy %>%
#  filter(
#    !is.na(urban_rural),
#    bird_type %in% top_birds
3  ) %>%
  mutate(pres = if_else(prest > 0, 1, 0)) %>%
  mutate_if(is.character, as.factor)

library(tidymodels)

set.seed(123)
mam_split <- initial_split(lista_tidy, strata = pres)
mam_train <- training(mam_split)
mam_test <- testing(mam_split)

set.seed(234)
mam_folds <- vfold_cv(mam_train, strata = pres)
mam_folds
  
  glm_spec <- logistic_reg() #math model


#transform our nominal (factor or character, like sp, UC) predictors to dummy or indicator variables.   
  rec_basic <-
  recipe(pres ~ area + sp + mNDVI, data = mam_train) %>% 	#feature engineering recipe formula
  step_dummy(sp)
  #step_dummy(all_nominal_predictors()) #indicar que todas variavel nominal da receita= dummy

#combine feature engineering recipe with math model
wf_basic <- workflow(rec_basic, glm_spec)

#para obter melhor estimativa da performance do modelo
doParallel::registerDoParallel()
ctrl_preds <- control_resamples(save_pred = TRUE)
rs_basic <- fit_resamples(wf_basic, mam_folds, control = ctrl_preds)

collect_metrics(rs_basic) #look

#augment(rs_basic) %>%
#  roc_curve(bird_count, .pred_bird) %>%
#  autoplot()

#Add interactions
#rec_interact <-
#  rec_basic %>%
#  step_interact(~ starts_with("urban_rural"):starts_with("bird_type"))

#wf_interact <- workflow(rec_interact, glm_spec)
#rs_interact <- fit_resamples(wf_interact, bird_folds, control = ctrl_preds)

#collect_metrics(rs_interact)



#We can fit the model one time to the entire training set.

#mam_fit <- fit(wf_interact, mam_train)
mam_fit <- fit(wf_basic, mam_train)

#we can predict the test set, perhaps to get out probabilities.

predict(mam_fit, mam_test, type = "prob") #olhe os nomes das colunas

#new_bird_data <-	#pra dividir cada sp em 2 linhas: urban e rural pra por os resultados de prob
#  tibble(bird_type = top_birds) %>%
#  crossing(urban_rural = c("Urban", "Rural"))

mam_preds <-
  augment(mam_fit) %>%
  bind_cols(
    predict(mam_fit, type = "conf_int")
  )

bird_preds #olhe nome das colunas

p2 <-
  mam_preds %>%
  ggplot(aes(area, sp, color = monkey)) + #criou uma coluna p dizer quais são macacos de interesse?
  geom_errorbar(aes(
    xmin = .pred_lower_bird, #olhou nome das colunas?
    xmax = .pred_upper_bird
  ),
  width = .2, size = 1.2, alpha = 0.5
  ) +
  geom_point(size = 2.5) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted probability of seeing mammal due to patch size", y = NULL, color = NULL)

p2


#####################################################################################################################

########## https://juliasilge.com/blog/superbowl-conf-int
########## model AREA por riqueza controlando por sp

simple_mod <- lm(year ~ funny + show_product_quickly +
  patriotic + celebrity + danger + animals + use_sex,
data = youtube
)
summary(simple_mod)

#reg_intervals() that finds confidence intervals for models like lm() and glm() (as well as models from the survival package).
set.seed(123)
youtube_intervals <- reg_intervals(year ~ funny + show_product_quickly +
  patriotic + celebrity + danger + animals + use_sex,
data = youtube,
type = "percentile",
keep_reps = TRUE
)
youtube_intervals

#If we had not set keep_reps = TRUE, we would only have the intervals themselves and could a plot such as this one.
youtube_intervals %>%
  mutate(
    term = str_remove(term, "TRUE"),
    term = fct_reorder(term, .estimate)
  ) %>%
  ggplot(aes(.estimate, term)) +
  geom_vline(xintercept = 0, size = 1.5, lty = 2, color = "gray80") +
  geom_errorbarh(aes(xmin = .lower, xmax = .upper),
    size = 1.5, alpha = 0.5, color = "midnightblue"
  ) +
  geom_point(size = 3, color = "midnightblue") +
  labs(
    x = "Increase in year for each commercial characteristic",
    y = NULL
  )

#ORDENACAO RESTRITA -> matriz de similaridade sp ~ ambientais
#data(varespec)
data(varechem)
## Common but bad way: use all variables you happen to have in your
## environmental data matrix
vare.cca <- cca(varespec, varechem)
vare.cca
plot(vare.cca)
## Formula interface and a better model
vare.cca <- cca(varespec ~ Al + P*(K + Baresoil), data=varechem)
vare.cca
plot(vare.cca)
## `Partialling out' and `negative components of variance'
cca(varespec ~ Ca, varechem)
cca(varespec ~ Ca + Condition(pH), varechem)
## RDA
data(dune)
data(dune.env)
dune.Manure <- rda(dune ~ Manure, dune.env)
plot(dune.Manure) 

#metadata do plot https://rdrr.io/cran/MVar.pt/man/Plot.CCA.html


#### Plotting multiple response variables in ggplot2 ###

#https://www.jscarlton.net/post/2017-04-05multipledotsggplot/

#set up your data so that each model run is a different observation (i.e., row), like this:
predictor   model   odds
pred_a      1       2.23
pred_a      2       1.32
pred_a      3       1.23
pred_b      1       0.82
pred_b      2       0.98
pred_c      3       0.98

library(tidyverse)
#create a fake dataset. 
df <- read_csv(
  "predictor, response, odds, CIHigh, CILow
  Predictor A, response 1, 2.23, 0.70, 6.60
  Predictor A, response 2, 1.32, 1.02, 1.70
  Predictor A, response 3, 1.23, 0.97, 1.56
  Predictor B, response 1, 0.82, 0.65, 1.04
  Predictor B, response 2, 0.98, 0.96, 1.00
  Predictor B, response 3, 0.98, 0.86, 1.11
  Predictor C, response 1, 0.66, 0.50, 0.87
  Predictor C, response 2, 0.59, 0.36, 0.98
  Predictor C, response 3, 0.98, 0.86, 1.11"
)

#A- plot multiple graphs NEXT TO EACH OTHER for easy comparison.
#facet_wrap to create the plots and coord_trans to transform the x axis to a log scale
ggplot(df, aes(x = odds, y = predictor)) +
  geom_vline(aes(xintercept = 1), size = .25, linetype = "dashed") +
  geom_errorbarh(aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50") +
  geom_point(size = 4, color = "blue") +
  facet_wrap(~response) +
  scale_x_continuous(breaks = seq(0,7,1) ) +
  coord_trans(x = "log10") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())


#B- plot ALL OF THE NODELS ON 1 graph and use color and position_nudge to differentiate between them
#using filter to plot one set of points and CIs at a time and manually adjusting their height using an adjustment variable and position_nudge()

adj = .2 # This is used in position_nudge to move the dots

ggplot(df, aes(x = odds, y = predictor, color = response)) +
  geom_vline(aes(xintercept = 1), size = .25, linetype = "dashed") +
  geom_errorbarh(data = filter(df, response== "response 1"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50", position = position_nudge(y = adj)) +
  geom_point(data = filter(df, response== "response 1"), size = 4, position = position_nudge(y = adj)) +
  geom_errorbarh(data = filter(df, response== "response 2"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50") +
  geom_point(data = filter(df, response== "response 2"), size = 4) +
  geom_errorbarh(data = filter(df, response== "response 3"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50", position = position_nudge(y = - adj)) +
  geom_point(data = filter(df, response== "response 3"), size = 4, position = position_nudge(y = - adj)) +
  scale_x_continuous(breaks = seq(0,7,1) ) +
  coord_trans(x = "log10") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())
  
  
