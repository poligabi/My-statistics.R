
### Starting with tidyverse Books:
#https://r4ds.had.co.nz/data-import.html
#https://www.tidymodels.org/start/models/
#https://moderndive.com/4-tidy.html
#https://supervised-ml-course.netlify.app/

######## TIDY DATA (transformando colunas excessivas de uma mesma variável em 2 coluna, 1 com os valores e outra com os nomes das colunas
data_tidy <- data %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "score",   #conterá os valores
               values_drop_na = TRUE  #expliciar que há ausencias nos dados
               cols = -local,         #tidy todas as colunas menos essa ou:
               #cols = c(1984:2021),
               names_transform = list(year = as.integer)) 
  #Há tbm o inverso, se você coloca uma coluna para tipos de variáveis (ex:escalas) que deveriam ter cada uma sua própia coluna
  table2 %>%
    pivot_wider(names_from = escalas, values_from = count)

####### filtrando              
drinks_smaller <- drinks %>% 
  filter(country %in% c("USA", "China", "Italy", "Saudi Arabia")) %>% #filtra so esses países
  select(-total_litres_of_pure_alcohol) %>%                           #seleciona todas col menos essa
  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)      #renomeia pra nome mais facil

###### To COMBINE the tidied versions of table4a and table4b into a single tibble dplyr::left_join()
tidy4a <- table4a %>%  #primeiro tidied
  pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "cases")
tidy4b <- table4b %>% 
  pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "population")
left_join(tidy4a, tidy4b, by="local") #depois combina
      #it preserves the original observations even when there isn’t a match. The left join should be your default join  
  ####### RELATIONAL DATA (multiplas tabelas com dados relacionados ex: matrix de sp e de ambientais
  #https://r4ds.had.co.nz/relational-data.html
  ### KEY variavel que identifica a observação unica
#An inner join keeps observations that appear in both tables
x %>% 
  inner_join(y, by = "key")
#A left join keeps all observations in x(add NA on col y). Default pois add mantendo a quantidade de linhas original
#A right join keeps all observations in y(add NA on col x).
#A full join keeps all observations in x and y (add lines to do it).
#by = NULL, uses all variables that appear in both tables, the so called natural join (default)
#by = c("a" = "b"). This will match variable a in table x to variable b in table y (2col que representam =var mas com nomes =!)


###############################################################################################################

######## https://juliasilge.com/blog/crop-yields/
######## How mammals richness are changing|vary with changing NDVI
#chamando dados
dt500<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt1m<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt2m<- read.table(file = "clipboard", sep = "\t", header=TRUE)
dt5m<- read.table(file = "clipboard", sep = "\t", header=TRUE) #com coluna Nsp
lista<- read.table(file = "clipboard", sep = "\t", header=TRUE)
str(lista) #4=UC_area 8=Nsp
#sp<-lista[,9:109]
#riq<-lista[,c(7,8)]

library(tidyverse)

############################### Tidy the serial data

dt500_td <- dt500 %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "500m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt1m_td <- dt1m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "1000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt2m_td <- dt2m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "2000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(,X1984:X2021))
               #names_transform = list(year = as.integer)) 
dt5m_td <- dt5m %>% 
  pivot_longer(names_to = "year",     #conterá os nomes das colunas orig
               values_to = "5000m",   #conterá os valores
               values_drop_na = TRUE,  #expliciar que há ausencias nos dados
               cols = c(,X1984:X2021))
               #names_transform = list(year = as.integer)) 
data_td1<- left_join(dt500_td,dt1m_td) #depois combina
data_td2<- left_join(data_td1,dt2m_td)
data_td5<- left_join(data_td2,dt5m_td)
str(data_td5)
data_td5 <- data_td5 %>% mutate(year = as.integer(gsub("X", "", year))) #tira X dos numeros e torna dados numericos

serialdt <- data_td5 %>% 
  pivot_longer(names_to = "escalas",     #conterá os nomes das colunas orig
               values_to = "ndvi",   #conterá os valores
               cols = c("500m","1000m","2000m","5000m")) %>%
		filter( escalas %in% c("500m","1000m","2000m","5000m"),
			 !is.na(ndvi)) #filtro importante pro modelo a seguir
edit(serialdt)

###################################### Modelo Serial

library(tidymodels)#boom de pacotes conflituosos
library(modelr)#ou um ou outro

serial_lm<- serialdt %>%
  nest(yndvi = c(year, ndvi)) %>% #cria um tible pra cada conjunto de anos e ndvi
  mutate(model = map(yndvi, ~ lm(ndvi ~ year, data = .x)))
	#try() forca ignorar o erro, inviavibiliza criar o modelo
	#http://adv-r.had.co.nz/Exceptions-Debugging.html #olhar tbm tryCatch()

slopes <- serial_lm %>%
  mutate(coefs = map(model, tidy)) %>%
  unnest(coefs) %>%
  filter(term == "year") %>%            
  mutate(p.value = p.adjust(p.value))

################################### Gráficos
library(ggrepel)
slopes %>%
  ggplot(aes(estimate, Nsp, label="")) +  
  geom_vline(
    xintercept = 0, lty = 2,
    size = 1.5, alpha = 0.7, color = "gray50"
  ) +
  geom_point(aes(color = escalas), alpha = 0.8, size = 2.5, show.legend = FALSE) +
  scale_y_log10() +
  facet_wrap(~escalas) +            
  geom_text_repel(size = 3, family = "IBMPlexSans") +
  theme_light(base_family = "IBMPlexSans") +
  theme(strip.text = element_text(family = "IBMPlexSans-Bold", size = 12)) +
  labs(x = "increase in NDVI rate per year")

############## with mean NDVI




##############################################################################

#########https://juliasilge.com/blog/bird-baths/

lista_tidy <- data %>% 
  pivot_longer(names_to = "sp",     #conterá os nomes das colunas orig
               values_to = "pres",   #conterá os valores
               values_drop_na = TRUE  #expliciar que há ausencias nos dados
               #cols = -local,         #tidy todas as colunas menos essa ou:
               cols = c(sp1:spx))
#tipoUCN ou preaus medium monkey 
lista_df <-
  lista_tidy %>%
#  filter(
#    !is.na(urban_rural),
#    bird_type %in% top_birds
3  ) %>%
  mutate(pres = if_else(prest > 0, 1, 0)) %>%
  mutate_if(is.character, as.factor)

library(tidymodels)

set.seed(123)
mam_split <- initial_split(lista_tidy, strata = pres)
mam_train <- training(mam_split)
mam_test <- testing(mam_split)

set.seed(234)
mam_folds <- vfold_cv(mam_train, strata = pres)
mam_folds
  
  glm_spec <- logistic_reg() #math model


#transform our nominal (factor or character, like sp, UC) predictors to dummy or indicator variables.   
  rec_basic <-
  recipe(pres ~ area + sp + mNDVI, data = mam_train) %>% 	#feature engineering recipe formula
  step_dummy(sp)
  #step_dummy(all_nominal_predictors()) #indicar que todas variavel nominal da receita= dummy

#combine feature engineering recipe with math model
wf_basic <- workflow(rec_basic, glm_spec)

#para obter melhor estimativa da performance do modelo
doParallel::registerDoParallel()
ctrl_preds <- control_resamples(save_pred = TRUE)
rs_basic <- fit_resamples(wf_basic, mam_folds, control = ctrl_preds)

collect_metrics(rs_basic) #look

#augment(rs_basic) %>%
#  roc_curve(bird_count, .pred_bird) %>%
#  autoplot()

#Add interactions
#rec_interact <-
#  rec_basic %>%
#  step_interact(~ starts_with("urban_rural"):starts_with("bird_type"))

#wf_interact <- workflow(rec_interact, glm_spec)
#rs_interact <- fit_resamples(wf_interact, bird_folds, control = ctrl_preds)

#collect_metrics(rs_interact)



#We can fit the model one time to the entire training set.

#mam_fit <- fit(wf_interact, mam_train)
mam_fit <- fit(wf_basic, mam_train)

#we can predict the test set, perhaps to get out probabilities.

predict(mam_fit, mam_test, type = "prob") #olhe os nomes das colunas

#new_bird_data <-	#pra dividir cada sp em 2 linhas: urban e rural pra por os resultados de prob
#  tibble(bird_type = top_birds) %>%
#  crossing(urban_rural = c("Urban", "Rural"))

mam_preds <-
  augment(mam_fit) %>%
  bind_cols(
    predict(mam_fit, type = "conf_int")
  )

bird_preds #olhe nome das colunas

p2 <-
  mam_preds %>%
  ggplot(aes(area, sp, color = monkey)) + #criou uma coluna p dizer quais são macacos de interesse?
  geom_errorbar(aes(
    xmin = .pred_lower_bird, #olhou nome das colunas?
    xmax = .pred_upper_bird
  ),
  width = .2, size = 1.2, alpha = 0.5
  ) +
  geom_point(size = 2.5) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted probability of seeing mammal due to patch size", y = NULL, color = NULL)

p2


#####################################################################################################################

########## https://juliasilge.com/blog/superbowl-conf-int
########## model AREA por riqueza controlando por sp

simple_mod <- lm(year ~ funny + show_product_quickly +
  patriotic + celebrity + danger + animals + use_sex,
data = youtube
)
summary(simple_mod)

#reg_intervals() that finds confidence intervals for models like lm() and glm() (as well as models from the survival package).
set.seed(123)
youtube_intervals <- reg_intervals(year ~ funny + show_product_quickly +
  patriotic + celebrity + danger + animals + use_sex,
data = youtube,
type = "percentile",
keep_reps = TRUE
)
youtube_intervals

#If we had not set keep_reps = TRUE, we would only have the intervals themselves and could a plot such as this one.
youtube_intervals %>%
  mutate(
    term = str_remove(term, "TRUE"),
    term = fct_reorder(term, .estimate)
  ) %>%
  ggplot(aes(.estimate, term)) +
  geom_vline(xintercept = 0, size = 1.5, lty = 2, color = "gray80") +
  geom_errorbarh(aes(xmin = .lower, xmax = .upper),
    size = 1.5, alpha = 0.5, color = "midnightblue"
  ) +
  geom_point(size = 3, color = "midnightblue") +
  labs(
    x = "Increase in year for each commercial characteristic",
    y = NULL
  )

#ORDENACAO RESTRITA -> matriz de similaridade sp ~ ambientais
#data(varespec)
data(varechem)
## Common but bad way: use all variables you happen to have in your
## environmental data matrix
vare.cca <- cca(varespec, varechem)
vare.cca
plot(vare.cca)
## Formula interface and a better model
vare.cca <- cca(varespec ~ Al + P*(K + Baresoil), data=varechem)
vare.cca
plot(vare.cca)
## `Partialling out' and `negative components of variance'
cca(varespec ~ Ca, varechem)
cca(varespec ~ Ca + Condition(pH), varechem)
## RDA
data(dune)
data(dune.env)
dune.Manure <- rda(dune ~ Manure, dune.env)
plot(dune.Manure) 

#metadata do plot https://rdrr.io/cran/MVar.pt/man/Plot.CCA.html


#### Plotting multiple response variables in ggplot2 ###

#https://www.jscarlton.net/post/2017-04-05multipledotsggplot/

#set up your data so that each model run is a different observation (i.e., row), like this:
predictor   model   odds
pred_a      1       2.23
pred_a      2       1.32
pred_a      3       1.23
pred_b      1       0.82
pred_b      2       0.98
pred_c      3       0.98

library(tidyverse)
#create a fake dataset. 
df <- read_csv(
  "predictor, response, odds, CIHigh, CILow
  Predictor A, response 1, 2.23, 0.70, 6.60
  Predictor A, response 2, 1.32, 1.02, 1.70
  Predictor A, response 3, 1.23, 0.97, 1.56
  Predictor B, response 1, 0.82, 0.65, 1.04
  Predictor B, response 2, 0.98, 0.96, 1.00
  Predictor B, response 3, 0.98, 0.86, 1.11
  Predictor C, response 1, 0.66, 0.50, 0.87
  Predictor C, response 2, 0.59, 0.36, 0.98
  Predictor C, response 3, 0.98, 0.86, 1.11"
)

#A- plot multiple graphs NEXT TO EACH OTHER for easy comparison.
#facet_wrap to create the plots and coord_trans to transform the x axis to a log scale
ggplot(df, aes(x = odds, y = predictor)) +
  geom_vline(aes(xintercept = 1), size = .25, linetype = "dashed") +
  geom_errorbarh(aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50") +
  geom_point(size = 4, color = "blue") +
  facet_wrap(~response) +
  scale_x_continuous(breaks = seq(0,7,1) ) +
  coord_trans(x = "log10") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())


#B- plot ALL OF THE NODELS ON 1 graph and use color and position_nudge to differentiate between them
#using filter to plot one set of points and CIs at a time and manually adjusting their height using an adjustment variable and position_nudge()

adj = .2 # This is used in position_nudge to move the dots

ggplot(df, aes(x = odds, y = predictor, color = response)) +
  geom_vline(aes(xintercept = 1), size = .25, linetype = "dashed") +
  geom_errorbarh(data = filter(df, response== "response 1"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50", position = position_nudge(y = adj)) +
  geom_point(data = filter(df, response== "response 1"), size = 4, position = position_nudge(y = adj)) +
  geom_errorbarh(data = filter(df, response== "response 2"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50") +
  geom_point(data = filter(df, response== "response 2"), size = 4) +
  geom_errorbarh(data = filter(df, response== "response 3"), aes(xmax = CIHigh, xmin = CILow), size = .5, height = .1, color = "gray50", position = position_nudge(y = - adj)) +
  geom_point(data = filter(df, response== "response 3"), size = 4, position = position_nudge(y = - adj)) +
  scale_x_continuous(breaks = seq(0,7,1) ) +
  coord_trans(x = "log10") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())
  
  
