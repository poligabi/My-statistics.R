


#### To do list:
# 1 # GRID 10 x 10 km: Vetor > Investigar > grade vetorial : x=0.1000
# 2 # Cruzamento espacial da camada alvo (pontos de coleta/grid de entrevistas) com as camadas dos dados ambientais
# - Vetor > Gerenciar dados> Unir atributos por localização
# - podes transferir os dados brutos, utilizando a opção “Tomar atributos da primeira feição localizada”, 
# - ou transferir os dados sumarizados, média, soma, mínima, máxima ou mediana, utilizando a função “Tomar sumário de feições intersectantes”
# 3 # Modelagem Hierarquica de occupação
# 4 # Criar arquivos .tif ou .asc dos atributos ambientais do estado: declividade, densidade de humanos...
# - Polígonos em raster : complemento GDALtools "Raster > Conversion > Rasterizar (lembrar de salvar SRC)
# - Raster> extração > cortador: camada mascara=polígono base(alagoas)
# - Declividade: Raster > análise de terreno >“Camada com as elevações” =altitude; Fatorz=1
# 5 # Mapa


#########################################################################
# Polígono da ocorrência: 						#
# 1- Pontos presença em mapa de calor					#
# 2- Raster> extração > cortador: camada mascara=polígono base(alagoas)	#
# 3- Raster > calculadora raster : camada>0.9				#
# 4- Raster > Conversor >Poligonizar 					#
#########################################################################


########## MODELAGEM HIERARQUICA CPB ->OccuFP->Occu

data <- read.table(file = "clipboard", sep = "\t", header=TRUE)
str(data)
y <- as.matrix(data[,1:5]) # Grab 2006-9 monkeys det/nondet data
solos <- data[,"Solos_cor"]
chu <- data[,"chu"]
tmax <- data[,"Temp_max"]
tmin <- data[,"Temp_min"]
tmed <- data[,"Temp_med"]

# Correlacionar covariates
covs <- cbind(solos, chu, tmax, tmin, tmed)
par(mfrow = c(3,3))
for(i in 1:8){
hist(covs[,i], breaks = 50, col = "grey", main = colnames(covs)[i])
}
pairs(cbind(solos, chu, tmax, tmin, tmed))


# Load unmarked, format data (Occu=classique or OccuFP=false-positive) and summarize
library(unmarked)
umf1 <- unmarkedFrameOccuFP(y = y, siteCovs = data.frame(solos = solos, chu = chu, 
tmax = tmax, tmin = tmin, tmed = tmed), type = 3 )
#type 1: assume-se que o processo de detecção é idêntico aos modelos clássicos, onde as probabilidades de falsos negativos são estimadas, mas os falsos positivos não ocorrem. 
#type 2: podemos ter falsos negativos e falsos positivos. Tanto p (a real probabilidade de detecção) e fp (a - a probabilidade de observar falsos positivos) são estimados para as ocasiões em que este tipo de dados podem ocorrer. 
#type 3: assumimos que as observações podem incluir detecções confirmadas(=2) (assume-se que falsos positivos não ocorrem) e detecções incertas que podem ou não incluir falsos positivos. 
summary(umf1)

m1 <- occuFP(detformula = ~ 1, FPformula = ~1, stateformula = ~ 1, data = umf1) #modelo nulo
#Error in if (sum(type[2:3]) == 0) stop("Only type 1 data. No data types with false positives. Use occu instead.") 

umf <- unmarkedFrameOccu(y = y, siteCovs = data.frame(solos = solos, chu = chu, 
tmax = tmax, tmin = tmin, tmed = tmed))


# Fit a series of models for detection first and do model selection
summary(fm1 <- occu(~1 ~1, data=umf))
summary(fm2 <- occu(~1 ~tmin , data=umf))
summary(fm3 <- occu(~1 ~tmax, data=umf))
cbind(fm1@AIC, fm2@AIC, fm3@AIC)       # outra maneira de selecionar pelo AIC



########## MODELAGEM HIERARQUICA ENTREVISTAS >OccuFP->Occu

data <- read.table(file = "clipboard", sep = "\t", header=TRUE)
str(data)

quest <- read.table(file = "clipboard", sep = "\t", header=TRUE)
str(quest)

#lon.orig <- data[,"Lon"] # Unstandardised, original values of covariates
#lat.orig <- data[,"Lat"]
#alt.orig <- data[,"Alt"]
dec.orig <- data[,"Dec"]
veg.orig <- data[,"veg"]
agua <- data[,"agua"]
#agdis.orig <- data[,"agdis"]
#area.orig <- data[,"area"]
#par.orig <- data[,"par"]
#form.orig <- data[,"form"]
isol.orig <- data[,"isol"]
urbd.orig <- data[,"urbd"]
agrd.orig <- data[,"agrd"]
pecd.orig <- data[,"pecd"]
popq.orig <- data[,"popq"]

prego <- as.matrix(quest[,15:18]) # Monkeys det/nondet data

queca.orig <- as.matrix(quest[,7:10])
queco.orig <- as.matrix(quest[,11:14])

# Standardise covariates and mean-impute date and duration
# Compute means and standard deviations
means <- c(apply(cbind(lon.orig, lat.orig, alt.orig, dec.orig, veg.orig, agdis.orig, area.orig, par.orig, form.orig,  
isol.orig, urbd.orig, agrd.orig, pecd.orig, popq.orig), 2, mean))
sds <- c(apply(cbind(lon.orig, lat.orig, alt.orig, dec.orig, veg.orig, agdis.orig, area.orig, par.orig, form.orig,  
isol.orig, urbd.orig, agrd.orig, pecd.orig, popq.orig), 2, sd))

# Scale covariates
lon <- (lon.orig - means[1]) / sds[1]
lat <- (lat.orig - means[2]) / sds[2]
alt <- (alt.orig - means[3]) / sds[3]
dec <- (dec.orig - means[4]) / sds[4]
veg <- (veg.orig - means[5]) / sds[5]
agdis <- (agdis.orig - means[6]) / sds[6]
area <- (area.orig - means[7]) / sds[7]
par <- (par.orig - means[8]) / sds[8]
form <- (form.orig - means[9]) / sds[9]
isol <- (isol.orig - means[10]) / sds[10]
urbd <- (urbd.orig - means[11]) / sds[11]
agrd <- (agrd.orig - means[12]) / sds[12]
pecd <- (pecd.orig - means[13]) / sds[13]
popq <- (popq.orig - means[14]) / sds[14]


#### OU  >>>>>>     primeiro padroniza e depois chama um a um
#pais <-decostand(paisagem, method="standardize") 	#Dados padronizados
#lon.s <- pais[,"Lon"]
#lat.s <- pais[,"Lat"]
#alt.s <- pais[,"Alt"]
dec.s <- pais[,"Dec"]
veg.s <- pais[,"veg"]
agua.s <- pais[,"agua"]
#agdis.s <- pais[,"agdis"]
#area.s <- pais[,"area"]
#par.s <- pais[,"par"]
#form.s <- pais[,"form"]
isol.s <- pais[,"isol"]
urbd.s <- pais[,"urbd"]
agrd.s <- pais[,"agrd"]
pecd.s <- pais[,"pecd"]
popq.s <- pais[,"popq"]

#### Overview of covariates
covs <- cbind(elev.orig, forest.orig, date.orig, dur.orig)
par(mfrow = c(3,3))
for(i in 1:8){
hist(covs[,i], breaks = 50, col = "grey", main = colnames(covs)[i])
}
pairs(cbind(elev.orig, forest.orig, date.orig, dur.orig))


##### Load unmarked, format data and summarize
library(unmarked)
umf <- unmarkedFrameOccu(y = prego, siteCovs = data.frame(urbd = urbd, 
agrd = agrd, pecd = pecd, popq = popq), 
obsCovs = list(queca = queca.orig, queco = queco.orig), type = 2)
summary(umf)
# OR
umf <- unmarkedFrameOccuFP(y = prego, siteCovs = data.frame(urbd = urbd, 
agrd = agrd, pecd = pecd, popq = popq), 
obsCovs = list(queca = queca.orig, queco = queco.orig), type = 2)
summary(umf)
#type 1: assume-se que o processo de detecção é idêntico aos modelos clássicos, onde as probabilidades de falsos negativos são estimadas, mas os falsos positivos não ocorrem. 
#type 2: podemos ter falsos negativos e falsos positivos. Tanto p (a real probabilidade de detecção) e fp (a - a probabilidade de observar falsos positivos) são estimados para as ocasiões em que este tipo de dados podem ocorrer. 
#type 3: assumimos que as observações podem incluir detecções confirmadas(=2) (assume-se que falsos positivos não ocorrem) e detecções incertas que podem ou não incluir falsos positivos. 

# Fit a series of models for detection first and do model selection
summary(fm1 <- occu(~1 ~1, data=umf))
summary(fm2 <- occu(~queca ~1, data=umf))
summary(fm3 <- occu(~queco ~1, data=umf))
summary(fm4 <- occu(~queca+queco ~1, data=umf))

# Put the fitted models in a "fitList" = rank them by AIC
fms <- fitList("p(.)psi(.)" = fm1,
	"p(queca)psi(.)" = fm2,
	"p(queco)psi(.)" = fm3,
	"p(queca+queco)psi(.)" = fm4)
(ms <- modSel(fms))

# OR >>>>>> outra maneira de selecionar pelo AIC
cbind(fm1@AIC, fm2@AIC, fm3@AIC, fm4@AIC)       

### Continue with model fitting for occupancy, guided by AIC as we go

#Check effects of spatial location
summary(fm10 <- occu(~queca ~lon, data=umf))
summary(fm11 <- occu(~queca ~lat, data=umf))
summary(fm12 <- occu(~queca ~lon+lat, data=umf))
cbind(fm2@AIC, fm10@AIC, fm11@AIC, fm12@AIC)

#Check effects of water
summary(fm13 <- occu(~queca ~agua, data=umf))
summary(fm14 <- occu(~queca ~agdis, data=umf))
summary(fm15 <- occu(~queca ~agua+agdis, data=umf))
cbind(fm2@AIC, fm13@AIC, fm14@AIC, fm15@AIC)

#Check effects of size and format
summary(fm16 <- occu(~queca ~area, data=umf))
summary(fm17 <- occu(~queca ~par, data=umf))
summary(fm18 <- occu(~queca ~form, data=umf))
summary(fm19 <- occu(~queca ~area+par, data=umf))
summary(fm20 <- occu(~queca ~area+form, data=umf))
cbind(fm2@AIC, fm16@AIC, fm17@AIC, fm18@AIC, fm19@AIC, fm20@AIC)

#Check effects of declivity and vegetation cover 
summary(fm21 <- occu(~queca ~dec, data=umf))
summary(fm22 <- occu(~queca ~veg, data=umf))
summary(fm23 <- occu(~queca ~dec+veg, data=umf))
cbind(fm2@AIC, fm21@AIC, fm22@AIC, fm23@AIC)

# Check effects of Matriz
summary(fm30 <- occu(~queca ~isol, data=umf))
summary(fm31 <- occu(~queca ~urbd, data=umf))
summary(fm32 <- occu(~queca ~agrd, data=umf))
summary(fm33 <- occu(~queca ~pecd, data=umf))
summary(fm34 <- occu(~queca ~popq, data=umf))
summary(fm35 <- occu(~queca ~isol+popq, data=umf))
summary(fm36 <- occu(~queca ~isol+urbd, data=umf))
summary(fm37 <- occu(~queca ~isol+agrd, data=umf))
summary(fm38 <- occu(~queca ~isol+pecd, data=umf))
summary(fm39 <- occu(~queca ~popq+urbd, data=umf))
summary(fm40 <- occu(~queca ~popq+agrd, data=umf))
summary(fm41 <- occu(~queca ~popq+pecd, data=umf))
summary(fm42 <- occu(~queca ~isol+popq+urbd, data=umf))
summary(fm43 <- occu(~queca ~isol+popq+agrd, data=umf))
summary(fm44 <- occu(~queca ~isol+popq+pecd, data=umf))
summary(fm45 <- occu(~queca ~isol+popq+agrdd+pecd, data=umf))
cbind(fm2@AIC, fm30@AIC, fm31@AIC, fm32@AIC, fm33@AIC, fm34@AIC, fm35@AIC, fm36@AIC,
fm37@AIC, fm38@AIC, fm39@AIC, fm40@AIC, fm41@AIC, fm42@AIC, fm43@AIC, fm44@AIC, fm45@AIC)

#Check effect of best models interactions (mantém o que descobriu nas etapas anteriores e testa mais combinações)

summary(fm50 <- occu(~queca ~, data=umf))
cbind( ,fm50@AIC, )


############# CRIAR MAPAS DE DISTRIBUIÇÂO

####### Load the landscape data

library(raster)
library(rgdal)

setwd("")         # diretorio da pasta com os arquivos .tif 

bio1.tif <- raster("bio01_neotropic_50km_gcs_wgs84.tif")        # importar um arquivo .tif ou .asc no formato rasterlayer 
bio1.tif              # visualizar as propriedades do arquivo raster importado 
plot(bio1.tif)        # plot do raster 

# Podemos importar diversos arquivos separadamente, atribuindo cada um a uma variável 
#ou podemos utilizar a função 'stack'

list.files(pattern = ".tif")           # listar os nomes dos arquivos na pasta do diretorio 
tif <- list.files(pattern = ".tif") 
tif.bios <- stack(tif)                 # importar os arquivos .tif no formato rasterstack 
tif.bios
names(tif.bios) <- paste0("bio", 1:19)  # renomear os arquivos raster importados 
tif.bios 
plot(tif.bios)            # plot de todos dos raster 
plot(tif.bios[[c(2, 5, 17, 18)]], col = rainbow(100, .7))     # plot de alguns dos raster 


# Get predictions of occupancy prob for each 1km2 quadrat
newData <- data.frame(elev = (tif.bios$elevation - means[1])/sds[1], forest = (tif.bios$forest - means[2])/sds[2])
predpsi <- predict(fm20, type="state", newdata=newData)        #fm20 = melhor função occu do unMarked


# Define new data frame with coordinates and outcome to be plotted
PARAM <- data.frame(x = tif.bios$x, y = tif.bios$y, z = predpsi$Predicted)
r1 <- rasterFromXYZ(PARAM)     # convert into raster object

# Mask quadrats with elevation greater than 2250
elev <- rasterFromXYZ(cbind(tif.bios$x, tif.bios$y, tif.bios$elevation))
elev[elev > 2250] <- NA
r1 <- mask(r1, elev)

# Plot species distribution map (Fig. 10-14 left)
par(mfrow = c(1,2), mar = c(1,2,2,5))
mapPalette <- colorRampPalette(c("grey", "yellow", "orange", "red"))
plot(r1, col = mapPalette(100), axes = F, box = F, main = "Capuchin monkey distribution between 2006 and 2009")

#Plot landscape characteristics
lakes <- readOGR(".", "lakes")
rivers <- readOGR(".", "rivers")
border <- readOGR(".", "border")
plot(rivers, col = "dodgerblue", add = TRUE)
plot(border, col = "transparent", lwd = 1.5, add = TRUE)
plot(lakes, col = "skyblue", border = "royalblue", add = TRUE)

# Plot SE of the species distrbution map (Fig. 10-14 right)
r2 <- rasterFromXYZ(data.frame(x = tif.bios$x, y = tif.bios$y, z = predpsi$SE))
r2 <- mask(r2, elev)
plot(r2, col = mapPalette(100), axes = F, box = F, main = "Uncertainty map between 2006 and 2009")
plot(rivers, col = "dodgerblue", add = TRUE)
plot(border, col = "transparent", lwd = 1.5, add = TRUE)
plot(lakes, col = "skyblue", border = "royalblue", add = TRUE)
points(data$coordx, data$coordy, pch = "+", cex = 0.8)        #add pontos de coleta






###### DENDROGRAMA + HEATMAP
#(1)Chamar os dados e pacote
library(vegan)
library(gplots)
library(RColorBrewer)

setwd("C:/R/intro/")
poli<-read.table("data-dout.txt",header=T,sep="\t")
prob<-poli[,1:4] #probabilidades de ocupação
row.names(prob)<-poli$local #nomeou todas as linhas com essa coluna
prob<-prob[,-1]		#depois tira essa coluna p ficar só o data frame

##
# (2) Criar mapa de calor comparando entre fragmentos:

# usando probabilidades de ocupação entre sp
scaleyellowred<- colorRampPalette(c("lightyellow","red"),space="rgb")(100)
heatmap(as.matrix(prob),Rowv=NA,Colv=NA,col=scaleyellowred) #cria mapa de calor

maxab<-apply(prob,1,max) # 2=colum 1=linha max=funçao q quero aplicar:aqui selecionei os valores maximos de cada linha
maxab
n1<-names(which(maxab<0.4))	#seleciona fragm com menos prob de ocupação
prob.1<-prob[,-which(names(prob) %in% n1)]	#retira esses fragm
heatmap(as.matrix(prob.1),Rowv=NA,Colv=NA,col=scaleyellowred,
margins=c(5,6))		#cria novo mapa de calor só com fragm com maior prob

#(3)Transformação dos dados da paisagem
paisagem<-poli[,5:10]
pairs(paisagem) 	#correlações
row.names(paisagem)<-poli$local #nomeou todas as linhas com essa coluna

# ou log
pairs(log(paisagem)) #confere
lopais<-log(paisagem) 
# ou padronização
pais.pad<-decostand(paisagem, method="standardize") 	#ou Padronização dos dados
#
pais.pad<-scale(paisagem)
##
#(4) Criar matriz de distancia
pais.bray <-vegdist(pais.pad, method="bray") #Bray-curtis
##
#(5) calcular cluster e coeficiente de correlaçao cofenetico 
cluster<-hclust(pais.bray, method="average")
cluster.coph<-cophenetic(cluster)

#compara matriz cofenetica com a matriz euclidiana, mais perto de 1 = +semelhantes :D
cor(pais.bray, cluster.coph) #resultado= 0.8686003 #nao usar a padronização pro metodo bray
##
# (6) Gerar mapa de calor com cluster
heatmap(as.matrix(prob),Rowv=as.dendrogram(cluster),
Colv=NA,col=scaleyellowred, margins=c(5,20),cexCol=1, cexRow=0.5)
	#cex=tamanho da letra; margins= proporção das margens entre colunas e linhas

help(heatmap)



